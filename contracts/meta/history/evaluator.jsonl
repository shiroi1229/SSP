{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T14:18:30.052034+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T14:18:30.052394+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T14:18:46.113429+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T14:18:46.113819+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T14:20:47.239743+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T14:20:47.240177+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T15:14:10.337920+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T15:14:10.338308+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T15:14:18.471439+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T15:14:18.471819+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T15:14:33.672681+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T15:14:33.673075+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T15:47:56.132329+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T15:47:56.132720+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T16:07:10.214983+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T16:07:10.215349+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T16:14:13.259640+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T16:14:13.260010+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T16:19:51.642904+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T16:19:51.643282+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T16:24:50.904672+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T16:24:50.905042+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T16:29:07.905470+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T16:29:07.905841+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T16:30:53.055509+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T16:30:53.061381+00:00"}
{"name": "evaluator", "path": "contracts\\evaluator.yaml", "version": "1.0", "schema_version": "v3.0", "status": "unknown", "description": "Evaluates the quality of a generated response based on the context.", "timestamp": "2025-11-15T16:35:50.145210+00:00", "stats": {"input_count": 2, "output_count": 2, "required_inputs": 1, "required_outputs": 0}, "missing_fields": [], "inputs": [{"name": "short_term.response", "type": "str", "description": "The generated response to be evaluated.", "required": true}, {"name": "long_term.model_params", "type": "dict", "description": "Current model parameters for reference.", "required": false}], "outputs": [{"name": "mid_term.evaluation_score", "type": "float", "description": "The calculated quality score of the response (0.0 to 1.0).", "required": false}, {"name": "long_term.optimization_log", "type": "list", "description": "A log entry detailing the evaluation, for future optimization.", "required": false}], "context_index": {"inputs": {"short_term": ["response"], "long_term": ["model_params"]}, "outputs": {"mid_term": ["evaluation_score"], "long_term": ["optimization_log"]}}, "recorded_at": "2025-11-15T16:35:50.145593+00:00"}
