# Shiroi System Platform データフロー概要

## 0. 全体像

高レベルでは、データフローは次のレイヤに分かれます。

- **UI レイヤ（Next.js Frontend）**
  - ルート例: `/chat`, `/chat/[id]`, `/dashboard`, `/docs/roadmap/...`
  - Hooks/Components が REST API を叩き、状態を管理（`useChat`, `useEvaluation`, `useRAGContext` 等）
- **API レイヤ（FastAPI Backend）**
  - エントリポイント: `backend/main.py` → `FastAPI(app)`  
  - ルーター: `backend/api/*.py`（`chat`, `sessions`, `knowledge`, `roadmap`, `status` など）
- **オーケストレーションレイヤ**
  - メイン: `orchestrator/main.py::run_context_evolution_cycle`
  - コンポーネント: `ContextManager`, `RAGPipelineService`, `Generator`, `Evaluator`, `Learner`, `AutoRepairEngine` 等
- **LLM & 埋め込み & ベクタDB レイヤ**
  - LLM: `modules/llm.py`（Transformers / HTTP LLM ハイブリッド）
  - 埋め込み: `modules/embedding_utils.py` → LM Studio 等の `/v1/embeddings`
  - ベクタDB: Qdrant (`modules/rag_engine.py`)
- **永続化レイヤ**
  - PostgreSQL: `backend/db/models.py`（`session_logs`, `roadmap_items`, `sessions`, `messages`, 他）
  - ファイル: `logs/*`, `data/*`, `reports/*`
  - メモリストア: `backend/core/memory_store.SqlAlchemyMemoryStore` 等

以下、主要フローを個別に分解します。

---

## 1. チャットフロー（UI → REST API → Orchestrator → LLM/DB）

### 1.1 フロントエンド: `/chat` → `/chat/[id]`

- エントリ: `frontend/app/chat/page.tsx`
  - マウント時に `fetch("/api/sessions")` でセッション一覧を取得。
  - 既存セッションがあれば先頭の `id` へ `router.replace("/chat/{id}")`。
  - なければ `POST /api/sessions` で新規セッションを作成し、その `id` へ遷移。
- セッション画面: `frontend/app/chat/[id]/page.tsx`
  - `useChat(initialSessionId)` を呼び出し、次を実行:
    - `GET /api/sessions` → サイドバー用 `sessions` state。
    - `GET /api/sessions/{id}` → メインの `messages` state（`user`/`assistant` メッセージ配列）。

### 1.2 `useChat` Hook のフロー

ファイル: `frontend/hooks/useChat.ts`

- `refreshSessions()`:
  - `GET /api/sessions` → `Envelope[data]` → `SessionSummary[]` に正規化。
  - `activeSessionId` や `initialSessionId` に応じて、自動的にどのセッションをロードするか決定。
- `loadSession(sessionId)`:
  - `GET /api/sessions/{sessionId}` → `SessionDetail`（`SessionSummary` + `messages`）。
  - `messages` を `Message` 型（role, content, timestamps 等）に変換。
- `sendMessage()`:
  - ユーザー入力を `Message` として楽観的に `messages` に追加。
  - `POST /api/chat` に `{ user_input, session_id }` を投げる。
  - 応答 `Envelope[data]` から `data.session_id` または `data.message.session_id` を取得。
  - 取得した `session_id` で `loadSession` を再実行し、履歴をサーバー側ソースオブトゥルースに同期。
  - 送信直後にタイトル未設定のセッションであれば、ユーザー入力の先頭行からタイトルを推論し、`PATCH /api/sessions/{id}/title` で保存。

### 1.3 バックエンド: `/api/chat` エンドポイント

ファイル: `backend/api/chat.py`

- ルーター登録: `backend/main.py` → `app.include_router(chat.router, prefix="/api")`
- 型:
  - `ChatRequest`:  
    - `session_id?: string`  
    - `user_input: string`（制御文字を除去するバリデータ付き）
  - `ChatMessageResponse`: DB Message を REST 用に整形したもの。
  - `ChatTurnPayload`: `{ session_id: str, message: ChatMessageResponse }`
- フロー:
  1. `chat_endpoint` が `ChatRequest` を受け取り、クエリパラメータの `session_id`（※現状は非推奨）とマージして正規化。
  2. `_ensure_session(db, sanitized_session_id)` で `sessions` テーブルからセッションを取得／新規作成:
     - 既存セッションID → 存在しなければ 404。
     - 新規セッション → UUID 生成、`created_at`, `updated_at`, `last_activity_at` を現在時刻でセット。
  3. `DBMessage` として user メッセージを `messages` テーブルに保存し、`session.last_activity_at` / `updated_at` を更新。
  4. `orchestrator.run_chat_cycle(user_input=...)` を呼び出し、オーケストレータパイプラインを走らせる。
     - 失敗 or 空文字の場合は `_simple_reply` でフォールバック（簡単な日本語メッセージ）。
  5. assistant メッセージも `messages` テーブルに保存し、`session.last_activity_at` / `updated_at` / `title` を（必要に応じて）更新。
  6. `memory_store.save_record_to_db({ session_id, user_input, final_output, created_at })` で `session_logs` テーブルにも集約ログを保存。
  7. `ChatTurnPayload` を `Envelope.ok` でラップして返す。

---

## 2. セッション一覧フロー（/api/sessions）

ファイル: `backend/api/sessions.py`

- `GET /api/sessions`:
  - `Session` モデルを `updated_at DESC` で取得。
  - `messages` テーブルから各セッションの最新メッセージをサブクエリで取得し、`last_message_preview` に反映。
  - `SessionSummary` に変換し、`Envelope[List[SessionSummary]]` を返す。
- `GET /api/sessions/{id}`:
  - 1件の `Session` を取得し、該当 `session_id` の `Message` を `created_at ASC` で取得。
  - `SessionDetail`（`SessionSummary` + `messages: MessageResponse[]`）として返す。
- `POST /api/sessions`:
  - 新規 `Session` を UUID で作成し、`SessionSummary` を返す。
- `PATCH /sessions/{id}` / `/sessions/{id}/title`:
  - タイトル／説明／アーカイブ状態を更新し、`SessionSummary` を返す。

※ 現状の問題点として、実 DB の `sessions` テーブルが旧仕様であるため、このAPI群が PostgreSQL の `UndefinedColumn` を発生させる可能性がある。  
データフローとしては上記が理想設計であり、スキーマを揃えることで完成する。

---

## 3. オーケストレータパイプラインのデータフロー

ファイル: `orchestrator/main.py` 他

### 3.1 概要

`run_context_evolution_cycle(user_input: str)` は、以下のフェーズで構成される。

1. **コンポーネント初期化**
   - `ContextManager`（`orchestrator/context_manager.py`）
   - `RAGPipelineService`（`backend/core/services/rag_pipeline_service.py`）
   - `Generator`（`modules/generator.py`）
   - `Evaluator`（`modules/evaluator.py`）
   - `MemoryStore`（`backend/core/memory_store.py` / `SqlAlchemyMemoryStore`）
   - `Learner` / `AutoRepairEngine` / `PolicyScheduler` など

2. **入力取り込み**
   - `ContextManager` の短期メモリレイヤ（例: `short_term.user_input`）にユーザー入力を記録。
   - `ContextHistory` やログファイルにも入力を追記。

3. **RAG 検索**
   - `modules/rag_engine.RAGEngine` が Qdrant と PostgreSQL を使って関連コンテキストを検索。
   - 検索結果（ドキュメント群）を `rag_context` として `ContextManager` に格納。

4. **応答生成（Generator）**
   - `modules/generator.py` が `modules.llm.analyze_text` を呼ぶ。
   - `modules/llm.py` は以下の順でバックエンドを選択:
     - Simulation mode (`LLM_SIMULATION_MODE=true`) → 固定JSON。
     - Transformers backend (`TRANSFORMERS_MODEL` が定義されている場合) → HuggingFace `pipeline("text-generation")`。
     - HTTP backend → `LOCAL_LLM_API_URL` + `LLM_HTTP_CHAT_PATH` に対して OpenAI互換の `/chat/completions` リクエスト。
   - 生成された応答は `ContextManager` の `mid_term.generated_output` などに保存。

5. **評価フェーズ（Evaluator）**
   - `modules/evaluator.evaluate_output` が、`generated_output` + `rag_context` を元に評価プロンプトを組み立て、再度 `analyze_text` を呼ぶ。
   - 応答は JSON 形式が期待されるが、実際には Markdown や説明付き JSON も来るため、
     - `_strip_code_fences` / `_extract_json_payload` を通して JSON を抽出。
   - 抽出できない場合はデフォルト評価値を採用しつつ、エラーログを出力。
   - 評価結果は `ContextManager.mid_term.evaluation_score` / `evaluation_feedback` に格納され、最終的にメトリクスログや DB にも反映。

6. **学習・メモリ更新（Learner / MemoryStore）**
   - `MemoryStore.save_record_to_db` などを通じて、`session_logs` テーブルに
     - `user_input`
     - `final_output`
     - `evaluation_score`
     - `workflow_trace`（JSON）
     - `status_code` 等
   を保存。
   - `Learner` は必要に応じて `embedding_utils` を使い、`world_knowledge` や Qdrant のベクタに新しいデータを追加。

7. **終了処理**
   - 最終的な応答文字列を呼び出し元（`run_chat_cycle` → `chat_endpoint`）に返す。

---

## 4. LLM / 埋め込み / ベクタDB のデータフロー

### 4.1 LLM（modules/llm.py）

- 入力:
  - `text`（ユーザー入力や生成済み回答）
  - `prompt`（システムプロンプト／評価プロンプト）
  - `model_params_override`（エンドポイントやモデル名、パラメータの上書き）
- 経路:
  1. `.env` / `config/model_params.json` を読み込み、温度、top_p、max_tokens などを決定。
  2. Persona を `prompt` に付与（`apply_persona_to_prompt`）。
  3. プロバイダ選択:
     - `LLM_PROVIDER` or `model_params["llm_provider"]` → `TRANSFORMERS` / `HTTP` / `HYBRID`。
  4. Transformers:
     - `pipeline("text-generation", model=TRANSFORMERS_MODEL, device=...)`。
  5. HTTP:
     - `LOCAL_LLM_API_URL` + `LLM_HTTP_CHAT_PATH` へ OpenAI互換リクエスト。
     - `/v1/models` などでモデル一覧をプローブし、適切な `model` 名を選択。
- 出力:
  - 通常: LLM のテキスト応答。
  - エラー時: 統一された JSON エラー文字列（`source_model`, `trend`, `suggestion`）を返す（呼び出し側はこれを検知して fallback する前提）。

### 4.2 埋め込み（modules/embedding_utils.py）

- `get_embedding(text: str)`:
  - `EMBEDDING_MODEL` / `LOCAL_LLM_API_URL` などの設定を用いて、  
    LM Studio や OpenAI互換 `/v1/embeddings` に POST。
  - 返ってきた `embedding` ベクトルを Python の list/ndarray として扱い、Qdrant や Postgres に保存。

### 4.3 Qdrant ベクタDB（modules/rag_engine.py）

- 検索フロー:
  - クエリテキストを `get_embedding` でベクトル化。
  - Qdrant クライアントで該当コレクション（`QDRANT_COLLECTION`）から top-k ベクトルを検索。
  - 結果を RAG コンテキストとして `ContextManager` に格納、LLM への入力にマージ。

---

## 5. 永続化レイヤのデータフロー

### 5.1 PostgreSQL

主要テーブル:

- `session_logs`: セッション単位のログ（入力・出力・評価・ワークフローJSON・状態コードなど）。
- `sessions`: チャットセッションのメタ情報（ID、タイトル、タイムスタンプ、アーカイブ状態など）— 理想的には `Session` モデルのスキーマに揃える。
- `messages`: 各セッションに紐づく user/assistant メッセージ。
- `roadmap_items`: ロードマップエントリ（v0.1〜vX.X など）。
- `world_knowledge`: RAG 用ドキュメントベース（ベクトルとメタデータ）。

### 5.2 ファイルベースログ

- `logs/context_history.json`: 過去のコンテキスト履歴。
- `logs/context_evolution_snapshots/*`: コンテキストのスナップショット。
- `logs/meta_contracts/*`: メタコントラクト・リンクグラフなど。
- `logs/ssp_log_json.log`, `logs/feedback_loop.log`: システム全体の JSON ログ／フィードバックループログ。

### 5.3 メモリストア

- `backend/core/memory_store.py`:
  - `SqlAlchemyMemoryStore` が `session_logs` を通じて永続化。
  - 評価結果やメタ情報もここ経由で保存される。

---

## 6. 今後の可視化方針

- 本ドキュメントをベースに、以下のような図式化を行う:
  - **レイヤ別コンポーネント図**：Frontend → Backend API → Orchestrator → LLM/DB の箱と矢印。
  - **シーケンス図**：
    - `/chat/[id]` での 1 ラウンド（ユーザー入力〜レスポンス）の時系列メッセージフロー。
    - Evaluator が LLM に再度問い合わせるフロー。
  - **データ永続化フロー図**：
    - `Message` / `Session` / `SessionLog` の関係、Qdrant との同期ポイント。

- 追加で、`docs/architecture/` 配下に:
  - `data_flow_overview.md`（このファイル）
  - `data_flow_chat_sequence.md`（チャット1ラウンドの詳細シーケンス）
  - `data_flow_rag_pipeline.md`（RAG & Evaluator の詳細）

などに分割すると、Copilot や他の開発者が特定部分だけを理解しやすくなる。

