# path: cli/log_analyzer.py
# version: v1

import os, json, argparse, datetime

LOG_DIR = os.getenv("SSP_LOG_DIR", "logs")

def list_logs():
    files = sorted(os.listdir(LOG_DIR), reverse=True)[:10]
    for f in files:
        print(f"ğŸ“„ {f}")

def search_logs(keyword):
    for f in os.listdir(LOG_DIR):
        path = os.path.join(LOG_DIR, f)
        with open(path, encoding="utf-8") as file:
            content = file.read()
            try:
                parsed_content = json.loads(content)
                # JSONãŒãƒªã‚¹ãƒˆã®å ´åˆã¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã‚’è€ƒæ…®
                if isinstance(parsed_content, list):
                    for item in parsed_content:
                        # å„ã‚¢ã‚¤ãƒ†ãƒ ã®æ–‡å­—åˆ—è¡¨ç¾ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        if keyword in json.dumps(item, ensure_ascii=False):
                            print(f"ğŸ” {f}")
                            text_to_print = json.dumps(item.get("data", item), ensure_ascii=False, indent=2)
                            for line in text_to_print.splitlines():
                                print(f"  {line}")
                            print()
                            break # æœ€åˆã®ãƒãƒƒãƒã§è¡¨ç¤ºã—ã€æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¸
                else:
                    # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã€ãã®æ–‡å­—åˆ—è¡¨ç¾ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if keyword in content:
                        print(f"ğŸ” {f}")
                        text_to_print = json.dumps(parsed_content.get("data", parsed_content), ensure_ascii=False, indent=2)
                        for line in text_to_print.splitlines():
                            print(f"  {line}")
                        print()
            except (json.JSONDecodeError, UnicodeDecodeError):
                # JSONå½¢å¼ã§ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue

def summarize_logs(count_only=False):
    count = 0
    dates = {}
    for f in os.listdir(LOG_DIR):
        path = os.path.join(LOG_DIR, f)
        with open(path, encoding="utf-8") as file:
            try:
                entry = json.load(file)
                if isinstance(entry, list):
                    for item in entry:
                        if "timestamp" in item:
                            date = item["timestamp"][:10]
                            dates[date] = dates.get(date, 0) + 1
                            count += 1
                else:
                    if "timestamp" in entry:
                        date = entry["timestamp"][:10]
                        dates[date] = dates.get(date, 0) + 1
                        count += 1
            except (json.JSONDecodeError, UnicodeDecodeError):
                continue # JSONå½¢å¼ã§ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
    
    if count_only:
        print(count)
    else:
        print(f"ğŸ“Š Total Logs: {count}")
        for d, c in sorted(dates.items()):
            print(f"  {d}: {c} entries")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SSP Development Log Analyzer")
    parser.add_argument("--list", action="store_true")
    parser.add_argument("--search", type=str)
    parser.add_argument("--summary", action="store_true")
    parser.add_argument("--count", action="store_true")
    args = parser.parse_args()

    if args.list:
        list_logs()
    elif args.search:
        search_logs(args.search)
    elif args.count:
        summarize_logs(count_only=True)
    elif args.summary:
        summarize_logs()
    else:
        parser.print_help()